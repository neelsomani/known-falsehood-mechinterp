# Do Models Distinguish Known Falsehoods?

We investigate whether language models represent a distinction between a proposition's truth value and its epistemic status in context. Specifically, we ask whether internal activations differ between (i) presenting a false proposition as a claim ("X") and (ii) presenting the same proposition as explicitly negated ("The following is false: X"), even when the surface content of X is identical. We construct a controlled dataset of short factual statements in three classes—true, false, and declared-false—using minimal lexical edits and diverse stance templates to reduce format artifacts. We record residual stream activations at multiple token positions and layers, and train linear probes to discriminate false vs declared-false. To test whether any separable representation is causally involved in reasoning, we ablate the inferred "stance direction" (and/or top-weight neurons) and measure changes in both probe performance and downstream behavior: truth-judgments, consistency across multi-step chains, and whether the model draws consequences from X versus ¬X under declared-false framing. Our results provide evidence that LMs can encode epistemic stance separately from propositional content, and that manipulating this representation can selectively disrupt the model's ability to treat explicit negation as a constraint on reasoning.
